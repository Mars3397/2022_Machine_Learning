{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3: Decision Tree, AdaBoost and Random Forest\n",
    "In hw3, you need to implement decision tree, adaboost and random forest by using only numpy, then train your implemented model by the provided dataset. TA will use the on-hold test label to evaluate your model performance.\n",
    "\n",
    "Please note that only **NUMPY** can be used to implement your model, you will get no points by simply calling `sklearn.tree.DecisionTreeClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Gini Index or Entropy is often used for measuring the “best” splitting of the data. Please compute the Entropy and Gini Index of provided data. Please use the formula from [page 5 of hw3 slides](https://docs.google.com/presentation/d/1kIe_-YZdemRMmr_3xDy-l0OS2EcLgDH7Uan14tlU5KE/edit#slide=id.gd542a5ff75_0_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "# Copy and paste your implementations right here to check your result\n",
    "# (Of course you can add your classes not written here)\n",
    "def gini(sequence):\n",
    "    c1, c2, l = 0, 0, len(sequence)\n",
    "    for i in range(l):\n",
    "        if sequence[i] == 1:    # count class 1\n",
    "            c1 += 1\n",
    "        else:                   # count class 2\n",
    "            c2 += 1\n",
    "    p1, p2 = c1 / l, c2 / l     # count prob. of each class\n",
    "    return 1 - p1 ** 2 - p2 ** 2\n",
    "\n",
    "def entropy(sequence):\n",
    "    c1, c2, l = 0, 0, len(sequence)\n",
    "    for i in range(l):\n",
    "        if sequence[i] == 1:    # count class 1\n",
    "            c1 += 1\n",
    "        else:                   # count class 2\n",
    "            c2 += 1\n",
    "    p1, p2 = c1 / l, c2 / l     # count prob. of each class\n",
    "    if not p1 or not p2:        # all in same class\n",
    "        return 0\n",
    "    else:\n",
    "        return -p1 * np.log2(p1) - p2 * np.log2(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 = class 1,\n",
    "# 2 = class 2\n",
    "data = np.array([1,2,1,1,1,1,2,2,1,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini of data is  0.4628099173553719\n"
     ]
    }
   ],
   "source": [
    "print(\"Gini of data is \", gini(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of data is  0.9456603046006401\n"
     ]
    }
   ],
   "source": [
    "print(\"Entropy of data is \", entropy(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "It is a binary classifiation dataset that classify if price is high or not for a cell phone, the label is stored in `price_range` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 21)\n",
      "(300, 21)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>battery_power</th>\n",
       "      <th>blue</th>\n",
       "      <th>clock_speed</th>\n",
       "      <th>dual_sim</th>\n",
       "      <th>fc</th>\n",
       "      <th>four_g</th>\n",
       "      <th>int_memory</th>\n",
       "      <th>m_dep</th>\n",
       "      <th>mobile_wt</th>\n",
       "      <th>n_cores</th>\n",
       "      <th>...</th>\n",
       "      <th>px_height</th>\n",
       "      <th>px_width</th>\n",
       "      <th>ram</th>\n",
       "      <th>sc_h</th>\n",
       "      <th>sc_w</th>\n",
       "      <th>talk_time</th>\n",
       "      <th>three_g</th>\n",
       "      <th>touch_screen</th>\n",
       "      <th>wifi</th>\n",
       "      <th>price_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1583</td>\n",
       "      <td>1</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0.7</td>\n",
       "      <td>148</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>942</td>\n",
       "      <td>1651</td>\n",
       "      <td>1704</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>745</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>0.8</td>\n",
       "      <td>102</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>89</td>\n",
       "      <td>1538</td>\n",
       "      <td>2459</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>832</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>0.7</td>\n",
       "      <td>103</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>125</td>\n",
       "      <td>1504</td>\n",
       "      <td>1799</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1175</td>\n",
       "      <td>1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0.3</td>\n",
       "      <td>164</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>873</td>\n",
       "      <td>1394</td>\n",
       "      <td>1944</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>695</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0.6</td>\n",
       "      <td>196</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1649</td>\n",
       "      <td>1829</td>\n",
       "      <td>2855</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   battery_power  blue  clock_speed  dual_sim  fc  four_g  int_memory  m_dep  \\\n",
       "0           1583     1          2.1         1  11       0          14    0.7   \n",
       "1            745     1          0.6         1   5       0          35    0.8   \n",
       "2            832     0          0.7         1   2       1          39    0.7   \n",
       "3           1175     1          1.3         0   2       0          19    0.3   \n",
       "4            695     0          0.5         0  18       1          12    0.6   \n",
       "\n",
       "   mobile_wt  n_cores  ...  px_height  px_width   ram  sc_h  sc_w  talk_time  \\\n",
       "0        148        7  ...        942      1651  1704    17    13          2   \n",
       "1        102        8  ...         89      1538  2459    14     1         16   \n",
       "2        103        4  ...        125      1504  1799     5     2         11   \n",
       "3        164        7  ...        873      1394  1944     9     4          9   \n",
       "4        196        2  ...       1649      1829  2855    16    13          7   \n",
       "\n",
       "   three_g  touch_screen  wifi  price_range  \n",
       "0        1             0     1            1  \n",
       "1        1             1     0            0  \n",
       "2        1             0     1            0  \n",
       "3        1             1     0            0  \n",
       "4        1             1     1            1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "val_df = pd.read_csv('val.csv')\n",
    "print(train_df.shape)\n",
    "print(val_df.shape)\n",
    "\n",
    "# extract labels from dataset\n",
    "y_train, y_val = train_df['price_range'], val_df['price_range']\n",
    "x_train, x_val = train_df.drop(columns=['price_range']), val_df.drop(columns=['price_range'])\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Implement the Decision Tree algorithm (CART, Classification and Regression Trees) and trained the model by the given arguments, and print the accuracy score on the validation data. You should implement two arguments for the Decision Tree algorithm\n",
    "1. **criterion**: The function to measure the quality of a split. Your model should support `gini` for the Gini impurity and `entropy` for the information gain. \n",
    "2. **max_depth**: The maximum depth of the tree. If `max_depth=None`, then nodes are expanded until all leaves are pure. `max_depth=1` equals to split data once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    def __init__(self, criterion='gini', max_depth=None):\n",
    "        # initialzation\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "        self.criterion = None\n",
    "        self.feature_importances = [0] * 20\n",
    "\n",
    "        if criterion == 'gini':\n",
    "            self.criterion = gini\n",
    "        else:\n",
    "            self.criterion = entropy\n",
    "\n",
    "    def fit(self, x_data, y_data, rf=None): # this is the train function\n",
    "        self.rf = rf\n",
    "        # deal with different input type\n",
    "        if isinstance(x_data, pd.DataFrame):\n",
    "            dataset = x_data.values.tolist()\n",
    "        else:\n",
    "            dataset = x_data\n",
    "        # merge x_data and y_data\n",
    "        train_dataset = [list(dataset[i]) + [y_data[i]] for i in range(len(y_data))]\n",
    "        # build the tree\n",
    "        self.root = self.best_split(train_dataset)\n",
    "        self.split_tree(self.root, self.max_depth, 1)\n",
    "\n",
    "    def predict(self, x_data):\n",
    "        # deal with different input type\n",
    "        if isinstance(x_data, pd.DataFrame):\n",
    "            dataset = x_data.values.tolist()\n",
    "        else:\n",
    "            dataset = x_data\n",
    "        # return prediction of each data\n",
    "        ret = [self.predict_data(dataset[i], self.root) for i in range(len(x_data))]\n",
    "        return ret\n",
    "\n",
    "    def predict_data(self, data, node):\n",
    "        # split predict data with attribute and threshold of each node\n",
    "        if data[node['attribute']] <= node['threshold']:\n",
    "            if isinstance(node['left'], dict):\n",
    "                return self.predict_data(data, node['left'])\n",
    "            else:\n",
    "                return node['left']\n",
    "        else:\n",
    "            if isinstance(node['right'], dict):\n",
    "                return self.predict_data(data, node['right'])\n",
    "            else:\n",
    "                return node['right']\n",
    "\n",
    "    def test_split(self, attribute, threshold, dataset):\n",
    "        # split dataset base on given attribute and threshold\n",
    "        left, right = [], []\n",
    "        for i in range(len(dataset)):\n",
    "            if dataset[i][attribute] <= threshold:\n",
    "                left.append(dataset[i])\n",
    "            else:\n",
    "                right.append(dataset[i])\n",
    "        return left, right\n",
    "\n",
    "    def best_split(self, dataset):\n",
    "        b_purity, b_attribute, b_threshold, b_left, b_right = 10, 999, 9000, None, None\n",
    "        # generate random vector of random forest\n",
    "        if self.rf != None:\n",
    "            cols = np.random.choice(len(dataset[0]) - 1, int(self.rf), replace=False)\n",
    "        for attribute in range(len(dataset[0]) - 1):\n",
    "            # skip the attribute not in random vector of random forest\n",
    "            if self.rf != None and attribute not in cols:\n",
    "                continue\n",
    "            # find unique threshold for the attribute\n",
    "            thresholds = np.array([data[attribute] for data in dataset])\n",
    "            thresholds = np.unique(thresholds)\n",
    "            thresholds = np.sort(thresholds)\n",
    "            # pop maximum to prevent left or right = None\n",
    "            if len(thresholds) == 1:\n",
    "                continue\n",
    "            thresholds = thresholds[:-1]\n",
    "            # iterate through thresholds to find best split\n",
    "            for threshold in thresholds:\n",
    "                left, right = self.test_split(attribute, threshold, dataset)\n",
    "                split_purity = (self.criterion([row[-1] for row in left]) * len(left) + self.criterion([row[-1] for row in right]) * len(right)) / len(dataset)\n",
    "                if split_purity < b_purity:\n",
    "                    b_purity, b_attribute, b_threshold, b_left, b_right = split_purity, attribute, threshold, left, right\n",
    "        return {'attribute':b_attribute, 'threshold':b_threshold, 'left':b_left, 'right':b_right}\n",
    "\n",
    "    def leaf_pred(self, dataset):\n",
    "        # output the most class in dataset\n",
    "        c1, c2 = 0, 0\n",
    "        for i in range(len(dataset)):\n",
    "            if dataset[i][-1] == 0:\n",
    "                c1 += 1\n",
    "            else:\n",
    "                c2 += 1\n",
    "        if c1 > c2:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def check_same_class(self, dataset):\n",
    "        # check whether all data in the dataset are in the same class\n",
    "        c = dataset[0][-1]\n",
    "        for i in range(len(dataset)):\n",
    "            if dataset[i][-1] != c:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def split_tree(self, node, max_depth, cur_depth):\n",
    "        # recursivly split the tree\n",
    "        left, right = node['left'], node['right']\n",
    "        del(node['left'])\n",
    "        del(node['right'])\n",
    "        # left or right = None -> predict\n",
    "        if not left or not right:\n",
    "            node['left'] = node['right'] = self.leaf_pred(left + right)\n",
    "            return \n",
    "        # reach max depth -> predict\n",
    "        if max_depth != None and cur_depth >= max_depth:\n",
    "            node['left'], node['right'] = self.leaf_pred(left), self.leaf_pred(right)\n",
    "            return\n",
    "        \n",
    "        if self.check_same_class(left):             # in the same class -> predict\n",
    "            node['left'] = self.leaf_pred(left)\n",
    "        else:\n",
    "            node['left'] = self.best_split(left)    \n",
    "            if node['left']['attribute'] == 999:    # if can not split anymore -> predict\n",
    "                node['left'] = self.leaf_pred(left)\n",
    "            else:                                   # keep split\n",
    "                self.split_tree(node['left'], max_depth, cur_depth + 1)\n",
    "        \n",
    "        if self.check_same_class(right):            # in the same class -> predict\n",
    "            node['right'] = self.leaf_pred(right)\n",
    "        else:\n",
    "            node['right'] = self.best_split(right)\n",
    "            if node['right']['attribute'] == 999:   # if can not split anymore -> predict\n",
    "                node['right'] = self.leaf_pred(right)\n",
    "            else:                                   # keep split\n",
    "                self.split_tree(node['right'], max_depth, cur_depth + 1)\n",
    "            \n",
    "    def print_tree(self, node, depth=0):\n",
    "        # recursivly print the tree\n",
    "        if isinstance(node, dict):\n",
    "            print('%s[Attribute %d < %.3f]' % ((depth*'\\t', (node['attribute']), node['threshold'])))\n",
    "            self.print_tree(node['left'], depth+1)\n",
    "            self.print_tree(node['right'], depth+1)\n",
    "        else:\n",
    "            print('%s[%s]' % ((depth*'\\t', node)))\n",
    "\n",
    "    def compute_feature_importance(self, node):\n",
    "        # travel the tree to compute feature importance\n",
    "        if isinstance(node, dict):\n",
    "            self.feature_importances[node['attribute']] += 1\n",
    "            self.compute_feature_importance(node['left'])\n",
    "            self.compute_feature_importance(node['right'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1\n",
    "Using `criterion=gini`, showing the accuracy score of validation data by `max_depth=3` and `max_depth=10`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth = 3:  Accuracy of validation data = 0.9166666666666666\n",
      "max_depth = 10: Accuracy of validation data = 0.9366666666666666\n"
     ]
    }
   ],
   "source": [
    "clf_depth3 = DecisionTree(criterion='gini', max_depth=3)    # declare decision tree\n",
    "clf_depth3.fit(x_train, y_train)                            # fit it with train data\n",
    "y_pred_3 = clf_depth3.predict(x_val)                        # predict with validation data\n",
    "print(f\"max_depth = 3:  Accuracy of validation data = {accuracy_score(y_val, y_pred_3)}\")\n",
    "\n",
    "clf_depth10 = DecisionTree(criterion='gini', max_depth=10) \n",
    "clf_depth10.fit(x_train, y_train)                           \n",
    "clf_depth10.compute_feature_importance(clf_depth10.root) \n",
    "y_pred_10 = clf_depth10.predict(x_val)\n",
    "print(f\"max_depth = 10: Accuracy of validation data = {accuracy_score(y_val, y_pred_10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2\n",
    "Using `max_depth=3`, showing the accuracy score of validation data by `criterion=gini` and `criterion=entropy`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "criterion=gini:    Accuracy of validation data = 0.9166666666666666\n",
      "criterion=entropy: Accuracy of validation data = 0.93\n"
     ]
    }
   ],
   "source": [
    "clf_gini = DecisionTree(criterion='gini', max_depth=3)\n",
    "clf_gini.fit(x_train, y_train)\n",
    "y_pred = clf_gini.predict(x_val)\n",
    "print(f\"criterion=gini:    Accuracy of validation data = {accuracy_score(y_val, y_pred)}\")\n",
    "\n",
    "clf_entropy = DecisionTree(criterion='entropy', max_depth=3)\n",
    "clf_entropy.fit(x_train, y_train)\n",
    "y_pred = clf_entropy.predict(x_val)\n",
    "print(f\"criterion=entropy: Accuracy of validation data = {accuracy_score(y_val, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: Your decisition tree scores should over **0.9**. It may suffer from overfitting, if so, you can tune the hyperparameter such as `max_depth`\n",
    "- Note: You should get the same results when re-building the model with the same arguments,  no need to prune the trees\n",
    "- Hint: You can use the recursive method to build the nodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Plot the [feature importance](https://sefiks.com/2020/04/06/feature-importance-in-decision-trees/) of your Decision Tree model. You can get the feature importance by counting the feature used for splitting data.\n",
    "\n",
    "- You can simply plot the **counts of feature used** for building tree without normalize the importance. Take the figure below as example, outlook feature has been used for splitting for almost 50 times. Therefore, it has the largest importance\n",
    "\n",
    "![image](https://i2.wp.com/sefiks.com/wp-content/uploads/2020/04/c45-fi-results.jpg?w=481&ssl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot feature importance\n",
    "# extract column titles\n",
    "attribute_temp = train_df.columns[0:-1]\n",
    "attribute, importances = [], []\n",
    "\n",
    "# only plot the feature with importance > 0\n",
    "for i in range(len(clf_depth10.feature_importances)):\n",
    "    if clf_depth10.feature_importances[i] != 0:\n",
    "        attribute.append(attribute_temp[i])\n",
    "        importances.append(clf_depth10.feature_importances[i])\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.bar(attribute, importances)\n",
    "plt.xticks(np.arange(len(attribute)), attribute)\n",
    "plt.yticks(np.arange(max(importances)+1))\n",
    "plt.xlabel('Attributes')\n",
    "plt.ylabel('count')\n",
    "plt.title('Feature Importance')\n",
    "plt.savefig('myimage.svg', format='svg', dpi=1200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "implement the AdaBooest algorithm by using the CART you just implemented from question 2 as base learner. You should implement one arguments for the AdaBooest.\n",
    "1. **n_estimators**: The maximum number of estimators at which boosting is terminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost():\n",
    "    def __init__(self, n_estimators):\n",
    "        # initialize\n",
    "        self.n_estimators = n_estimators\n",
    "        self.alphas = []\n",
    "        self.clfs = []\n",
    "\n",
    "    def fit(self, x_data, y_data):\n",
    "        data_weight = np.full(len(x_data), (1 / len(x_data)))\n",
    "        for n in range(self.n_estimators):\n",
    "            # bootstrap with data weight\n",
    "            rows = np.random.choice(len(x_data), len(x_data), p=data_weight)\n",
    "            if isinstance(x_data, pd.DataFrame):\n",
    "                dataset, labels = x_data.iloc[rows], y_data.iloc[rows].values.tolist()\n",
    "            else:\n",
    "                dataset = [x_data[row] for row in rows]\n",
    "                labels  = [y_data[row] for row in rows]\n",
    "            clf = DecisionTree(criterion='gini', max_depth=1)   # declare a decision tree\n",
    "            clf.fit(dataset, labels)                            # fit with bootstrap data\n",
    "            clf_predict = clf.predict(x_data)                   # predict with \"all\" data\n",
    "            # compute error and alpha\n",
    "            error = 0\n",
    "            for i in range(len(rows)):\n",
    "                if clf_predict[i] != y_data[i]:\n",
    "                    error += data_weight[i]\n",
    "            alpha = 0.5 * np.log((1 - error) / error)\n",
    "            # update data weight\n",
    "            for i in range(len(rows)):\n",
    "                if y_data[i] == clf_predict[i]:\n",
    "                    data_weight[i] *= np.exp(-alpha)\n",
    "                else:\n",
    "                    data_weight[i] *= np.exp(alpha)\n",
    "            # normalize\n",
    "            data_weight /= sum(data_weight)\n",
    "            self.alphas.append(alpha)\n",
    "            self.clfs.append(clf)\n",
    "\n",
    "    def predict(self, x_data):\n",
    "        out = np.full(len(x_data), 0.0)\n",
    "        # compute wighted sum of each clf's prediction for each data\n",
    "        for i in range(self.n_estimators):\n",
    "            pred = self.clfs[i].predict(x_data)\n",
    "            for j in range(len(x_data)):\n",
    "                if pred[j] == 0:\n",
    "                    out[j] -= self.alphas[i]\n",
    "                else:\n",
    "                    out[j] += self.alphas[i]\n",
    "        # output the sign of each weighted prediction\n",
    "        out = np.sign(out)\n",
    "        # convert -1 to 0\n",
    "        for i in range(len(out)):\n",
    "            if out[i] == -1:\n",
    "                out[i] = 0\n",
    "        return np.asarray(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1\n",
    "Show the accuracy score of validation data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 10:  Accuracy of validation data = 0.94\n",
      "n_estimators = 100:  Accuracy of validation data = 0.9633333333333334\n"
     ]
    }
   ],
   "source": [
    "ada_10 = AdaBoost(n_estimators=10)\n",
    "ada_10.fit(x_train, y_train)\n",
    "y_pred = ada_10.predict(x_val)\n",
    "print(f\"n_estimators = 10:  Accuracy of validation data = {accuracy_score(y_val, y_pred)}\")\n",
    "\n",
    "ada_100 = AdaBoost(n_estimators=100)\n",
    "ada_100.fit(x_train, y_train)\n",
    "y_pred = ada_100.predict(x_val)\n",
    "print(f\"n_estimators = 100:  Accuracy of validation data = {accuracy_score(y_val, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "implement the Random Forest algorithm by using the CART you just implemented from question 2. You should implement three arguments for the Random Forest.\n",
    "\n",
    "1. **n_estimators**: The number of trees in the forest. \n",
    "2. **max_features**: The number of random select features to consider when looking for the best split\n",
    "3. **bootstrap**: Whether bootstrap samples are used when building tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    def __init__(self, n_estimators, max_features, boostrap=True, criterion='gini', max_depth=None):\n",
    "        # initialize\n",
    "        self.n_estimators = n_estimators\n",
    "        self.bootstrap = boostrap\n",
    "        self.criterion = criterion\n",
    "        self.max_features = max_features\n",
    "        self.clfs = []\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def fit(self, x_data, y_data):\n",
    "        for _ in range(self.n_estimators):\n",
    "            clf = DecisionTree(self.criterion, self.max_depth)\n",
    "            # bootstrap or not\n",
    "            if self.bootstrap == True:\n",
    "                rows = np.random.choice(len(x_data), len(x_data))\n",
    "                # deal with different input type\n",
    "                if isinstance(x_data, pd.DataFrame):\n",
    "                    dataset, labels = x_data.iloc[rows], y_data.iloc[rows].values.tolist()\n",
    "                else:\n",
    "                    dataset = [x_data[row] for row in rows]\n",
    "                    labels  = [y_data[row] for row in rows]\n",
    "            else:\n",
    "                dataset, labels = x_data, y_data\n",
    "            # fit clf with train data\n",
    "            clf.fit(dataset, labels, self.max_features)\n",
    "            self.clfs.append(clf)\n",
    "\n",
    "    def predict(self, x_data):\n",
    "        # predict all data with each clf\n",
    "        p = []\n",
    "        for i in range(self.n_estimators):\n",
    "            t = self.clfs[i].predict(x_data)\n",
    "            p.append(t)\n",
    "        # select most vote for each data\n",
    "        ans = []\n",
    "        for i in range(len(x_data)):\n",
    "            lt = [col[i] for col in p]\n",
    "            ans.append(max(set(lt), key=lt.count))\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.1\n",
    "Using `criterion=gini`, `max_depth=None`, `max_features=sqrt(n_features)`, showing the accuracy score of validation data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 10:  Accuracy of validation data = 0.94\n",
      "n_estimators = 100:  Accuracy of validation data = 0.9466666666666667\n"
     ]
    }
   ],
   "source": [
    "clf_10tree = RandomForest(n_estimators=10, max_features=np.sqrt(x_train.shape[1]))\n",
    "clf_10tree.fit(x_train, y_train)\n",
    "y_pred = clf_10tree.predict(x_val)\n",
    "print(f\"n_estimators = 10:  Accuracy of validation data = {accuracy_score(y_val, y_pred)}\")\n",
    "\n",
    "clf_100tree = RandomForest(n_estimators=100, max_features=np.sqrt(x_train.shape[1]))\n",
    "clf_100tree.fit(x_train, y_train)\n",
    "y_pred = clf_100tree.predict(x_val)\n",
    "print(f\"n_estimators = 100:  Accuracy of validation data = {accuracy_score(y_val, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.2\n",
    "Using `criterion=gini`, `max_depth=None`, `n_estimators=10`, showing the accuracy score of validation data by `max_features=sqrt(n_features)` and `max_features=n_features`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random feature: Accuracy of validation data = 0.93\n",
      "all feature:    Accuracy of validation data = 0.9633333333333334\n"
     ]
    }
   ],
   "source": [
    "clf_random_features = RandomForest(n_estimators=10, max_features=np.sqrt(x_train.shape[1]))\n",
    "clf_random_features.fit(x_train, y_train)\n",
    "y_pred = clf_random_features.predict(x_val)\n",
    "print(f\"random feature: Accuracy of validation data = {accuracy_score(y_val, y_pred)}\")\n",
    "\n",
    "clf_all_features = RandomForest(n_estimators=10, max_features=x_train.shape[1])\n",
    "clf_all_features.fit(x_train, y_train)\n",
    "y_pred = clf_all_features.predict(x_val)\n",
    "print(f\"all feature:    Accuracy of validation data = {accuracy_score(y_val, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: Use majority votes to get the final prediction, you may get slightly different results when re-building the random forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6. Train and tune your model on a real-world dataset\n",
    "Try you best to get higher accuracy score of your model. After parameter tuning, you can train your model on the full dataset (train + val).\n",
    "- Feature engineering\n",
    "- Hyperparameter tuning\n",
    "- Implement any other ensemble methods, such as gradient boosting. Please note that you **can not** call any package. Also, only ensemble method can be used. Neural network method is not allowed to used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_your_model(data):\n",
    "    ## Define your model and training \n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        data = data.values.tolist()\n",
    "    x_data, y_data = [row[0:-1] for row in data], [row[-1] for row in data]\n",
    "    clf = AdaBoost(n_estimators=150)\n",
    "    clf.fit(x_data, y_data)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = train_your_model(pd.concat([train_df, val_df]))\n",
    "\n",
    "x_test = pd.read_csv('x_test.csv')\n",
    "y_pred = my_model.predict(x_test)\n",
    "\n",
    "with open('model.pickle', 'wb') as pkl_file:\n",
    "    pickle.dump(my_model, pkl_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "np.save(\"y_pred.npy\", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert y_pred.shape == (500, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary\n",
    "If you have trouble to implement this homework, TA strongly recommend watching [this video](https://www.youtube.com/watch?v=LDRbO9a6XPU), which explains Decision Tree model clearly. But don't copy code from any resources, try to finish this homework by yourself! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DO NOT MODIFY CODE BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'y_test.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/guoyun/Desktop/機器學習/CS_CS20024/HW3/109550018_HW3.ipynb Cell 35\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/guoyun/Desktop/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92/CS_CS20024/HW3/109550018_HW3.ipynb#X46sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/guoyun/Desktop/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92/CS_CS20024/HW3/109550018_HW3.ipynb#X46sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy_score\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/guoyun/Desktop/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92/CS_CS20024/HW3/109550018_HW3.ipynb#X46sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m y_test \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39my_test.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)[\u001b[39m'\u001b[39m\u001b[39mprice_range\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/guoyun/Desktop/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92/CS_CS20024/HW3/109550018_HW3.ipynb#X46sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTest-set accuarcy score: \u001b[39m\u001b[39m'\u001b[39m, accuracy_score(y_test, y_pred))\n",
      "File \u001b[0;32m~/.pyenv/versions/miniforge3/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/miniforge3/lib/python3.9/site-packages/pandas/util/_decorators.py:317\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    312\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    313\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    314\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    315\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(inspect\u001b[39m.\u001b[39mcurrentframe()),\n\u001b[1;32m    316\u001b[0m     )\n\u001b[0;32m--> 317\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/miniforge3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/.pyenv/versions/miniforge3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.pyenv/versions/miniforge3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/.pyenv/versions/miniforge3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1729\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1727\u001b[0m     is_text \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1728\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1729\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1730\u001b[0m     f,\n\u001b[1;32m   1731\u001b[0m     mode,\n\u001b[1;32m   1732\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1733\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1734\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1735\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1736\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1737\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1738\u001b[0m )\n\u001b[1;32m   1739\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1740\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.pyenv/versions/miniforge3/lib/python3.9/site-packages/pandas/io/common.py:857\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    853\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    855\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    856\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    858\u001b[0m             handle,\n\u001b[1;32m    859\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    860\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    861\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    862\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    863\u001b[0m         )\n\u001b[1;32m    864\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    865\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    866\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'y_test.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_test = pd.read_csv('y_test.csv')['price_range'].values\n",
    "\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** We will check your result for Question 3 manually *** (5 points)\n",
      "*** We will check your result for Question 6 manually *** (20 points)\n",
      "Approximate score range: 45.0 ~ 70.0\n",
      "*** This score is only for reference ***\n"
     ]
    }
   ],
   "source": [
    "def discrete_checker(score, thres, clf, name, x_train, y_train, x_test, y_test):\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    if accuracy_score(y_test, y_pred) - thres >= 0:\n",
    "        return score\n",
    "    else:\n",
    "        print(f\"{name} failed\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "def patient_checker(score, thres, CLS, kwargs, name,\n",
    "                    x_train, y_train, x_test, y_test, patient=10):\n",
    "    while patient > 0:\n",
    "        patient -= 1\n",
    "        clf = CLS(**kwargs)\n",
    "        clf.fit(x_train, y_train)\n",
    "        y_pred = clf.predict(x_test)\n",
    "        # print(accuracy_score(y_test, y_pred))\n",
    "        if accuracy_score(y_test, y_pred) - thres >= 0:\n",
    "            return score\n",
    "    print(f\"{name} failed\")\n",
    "    print(\"Considering the randomness, we will check it manually\")\n",
    "    return 0\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "    file_url = \"http://storage.googleapis.com/download.tensorflow.org/data/abalone_train.csv\"\n",
    "    df = pd.read_csv(\n",
    "        file_url,\n",
    "        names=[\"Length\", \"Diameter\", \"Height\", \"Whole weight\", \"Shucked weight\",\n",
    "               \"Viscera weight\", \"Shell weight\", \"Age\"]\n",
    "    )\n",
    "\n",
    "    df['Target'] = (df[\"Age\"] > 15).astype(int)\n",
    "    df = df.drop(labels=[\"Age\"], axis=\"columns\")\n",
    "\n",
    "    train_idx = range(0, len(df), 10)\n",
    "    test_idx = range(1, len(df), 20)\n",
    "\n",
    "    train_df = df.iloc[train_idx]\n",
    "    test_df = df.iloc[test_idx]\n",
    "\n",
    "    x_train = train_df.drop(labels=[\"Target\"], axis=\"columns\")\n",
    "    feature_names = x_train.columns.values\n",
    "    x_train = x_train.values\n",
    "    y_train = train_df['Target'].values\n",
    "\n",
    "    x_test = test_df.drop(labels=[\"Target\"], axis=\"columns\")\n",
    "    x_test = x_test.values\n",
    "    y_test = test_df['Target'].values\n",
    "    return x_train, y_train, x_test, y_test, feature_names\n",
    "\n",
    "\n",
    "score = 0\n",
    "\n",
    "data = np.array([1, 2])\n",
    "if abs(gini(data) - 0.5) < 1e-4:\n",
    "    score += 2.5\n",
    "else:\n",
    "    print(\"gini test failed\")\n",
    "\n",
    "if abs(entropy(data) - 1) < 1e-4:\n",
    "    score += 2.5\n",
    "else:\n",
    "    print(\"entropy test failed\")\n",
    "\n",
    "x_train, y_train, x_test, y_test, feature_names = load_dataset()\n",
    "\n",
    "score += discrete_checker(5, 0.9337,\n",
    "                          DecisionTree(criterion='gini', max_depth=3),\n",
    "                          \"DecisionTree(criterion='gini', max_depth=3)\",\n",
    "                          x_train, y_train, x_test, y_test\n",
    "                          )\n",
    "\n",
    "score += discrete_checker(2.5, 0.9036,\n",
    "                          DecisionTree(criterion='gini', max_depth=10),\n",
    "                          \"DecisionTree(criterion='gini', max_depth=10)\",\n",
    "                          x_train, y_train, x_test, y_test\n",
    "                          )\n",
    "\n",
    "score += discrete_checker(2.5, 0.9096,\n",
    "                          DecisionTree(criterion='entropy', max_depth=3),\n",
    "                          \"DecisionTree(criterion='entropy', max_depth=3)\",\n",
    "                          x_train, y_train, x_test, y_test\n",
    "                          )\n",
    "\n",
    "print(\"*** We will check your result for Question 3 manually *** (5 points)\")\n",
    "\n",
    "score += patient_checker(\n",
    "    7.5, 0.91, AdaBoost, {\"n_estimators\": 10},\n",
    "    \"AdaBoost(n_estimators=10)\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "score += patient_checker(\n",
    "    7.5, 0.87, AdaBoost, {\"n_estimators\": 100},\n",
    "    \"AdaBoost(n_estimators=100)\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "score += patient_checker(\n",
    "    5, 0.91, RandomForest,\n",
    "    {\"n_estimators\": 10, \"max_features\": np.sqrt(x_train.shape[1])},\n",
    "    \"RandomForest(n_estimators=10, max_features=sqrt(n_features))\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "score += patient_checker(\n",
    "    5, 0.91, RandomForest,\n",
    "    {\"n_estimators\": 100, \"max_features\": np.sqrt(x_train.shape[1])},\n",
    "    \"RandomForest(n_estimators=100, max_features=sqrt(n_features))\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "score += patient_checker(\n",
    "    5, 0.92, RandomForest,\n",
    "    {\"n_estimators\": 10, \"max_features\": x_train.shape[1]},\n",
    "    \"RandomForest(n_estimators=10, max_features=n_features)\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "print(\"*** We will check your result for Question 6 manually *** (20 points)\")\n",
    "print(\"Approximate score range:\", score, \"~\", score + 25)\n",
    "print(\"*** This score is only for reference ***\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e3ae70e51d0a81f5a9ce9a5aa0bda06ada15e9af7deae70d5f488fbd6e54dce1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
